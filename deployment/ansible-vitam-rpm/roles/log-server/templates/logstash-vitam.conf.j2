
# ----------------------------------------------------------------------
# fichier de config et de parsing logstash des logs VITAM
# Version logstash : logstash-2.3.4
#
# --------------------------------------
# date creation : Aug/08/2016
# date modif    : Sep/01/2016
# Auteur        : stephane.saadi@thalesgroup.com
#
# -----------------------------------------------------------------------

input {
         # KWA TODO : A supprimer quand ES enverra ses logs en syslog
        file {
                type => "import_elastic"
                path => ["{{elasticsearch_log_dir}}/{{cluster_name}}.log"]
                sincedb_path => "/tmp/data"
                #start_position => beginning
                codec => multiline {
                        pattern => "^\[%{TIMESTAMP_ISO8601:timestamp}\]"
                        negate => true
                        what => previous
                }
        }
        # KWA TODO : A étudier lorsqu'on changera la localisation ES
        file {
                type => "import_curator"
                path => "{{curator_log_dir}}/curator.log"
                sincedb_path => "/tmp/data"
                #start_position => beginning
        }

        # --- syslog input etc ...
        syslog {
                type => syslog_input
                port => 10514
        }
}
# -----------------------------------------------------
filter {
  mutate {
    gsub => [ "message", "\t", ' ' ]
    gsub => [ "message", "\n", '' ]
    gsub => [ "message", "\%2A", '*' ]
    gsub => [ "message", "\%3A", ':' ]
  }
  #---------------------------
  if [type] == "syslog_input" {
    grok {
      break_on_match => false
      keep_empty_captures => true
      # Keep only programs whose name starts with 'vitam-'
      match => [ "program", "(?:(vitam[-._])?)(?<prog_name>[^.]*)" ]
    }
    mutate {
      add_field => [ "type_input", "syslog" ]
    }
  }
  #---------------------------
  if [type] == "import_elastic" {
    mutate { add_field => [ "type_input", "%{type}" ] }
    grok {
      break_on_match => false
      keep_empty_captures => true
      match => [ "path", "(?:%{UNIXPATH:path_file})/(?<cluster_name>[^.]*)"]
      match => [ "path_file", "(?:%{UNIXPATH:path_log})/(?<program>[^.]*)"]
    }
  }
  if [type] == "import_curator" {
    mutate { add_field => [ "type_input", "%{type}" ] }
    grok {
      break_on_match => false
      keep_empty_captures => true
      match => [ "path", "(?:%{UNIXPATH:path_file})/(?<program>[^.]*)"]
    }
  }
  #---------------------------
  if [program] =~ "vitam-" {
      mutate { replace => { type => "logback" } }
  } else if [program] =~ "elastic" {
      mutate { replace => { type => "elastic" } }
  } else if [program] =~ "curator" {
      mutate { replace => { type => "curator" } }
  } else if [program] =~ "mongo" {
      mutate { replace => { type => "mongo" } }
  } else if [program] =~ "script" {
      mutate { replace => { type => "script" } }
  }
  # --------------------------------------------------
  if [type] =~ "logback" {
      grok {
        break_on_match => false
        keep_empty_captures => true

        # 2016-08-04 13:53:00,161 [main] INFO  fr.gouv.vitam.access.rest.AccessApplication - fr.gouv.vitam.access.rest.AccessApplication.startApplication(AccessApplication.java:122) : [access_1:access:425367] AccessApplication Starts on default
        # 2016-08-04 13:51:55,661 [main] INFO  org.godb.driver.cluster - com.godb.connection.SingleServerCluster.<init>(SingleServerCluster.java:45) : Cluster created with settings {hosts=[vitam-prod-app-17.internet.agri:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms', maxWaitQueueSize=500}

        match => [ "message", "%{TIMESTAMP_ISO8601:timestamp}(%{SPACE:space}) \[(?<module_name>[^\]]*)\] (?:%{SPACE:space}?)%{WORD:err_level}(?:%{SPACE:space}?)(?:%{GREEDYDATA:msg_data})"]
        match => [ "msg_data",
                   "(?<class_name>[^\b]*)%{SPACE:space}-%{SPACE:space}(?<module_acces>[^\b]*)(%{SPACE:space}) : (?:%{GREEDYDATA:msg_info})" ]

        overwrite => [ "timestamp" ]
      }
  }
  # --------------------------------------------------
  if [type] =~ "mongo" {
      grok {
        break_on_match => false
        keep_empty_captures => true
        match => [ "message", "(?:%{GREEDYDATA:msg_info})" ]
        # "timestamp" => "Aug 19 20:09:00",
        match => [ "timestamp", "(%{MONTH:mois} %{MONTHDAY:jour} %{HOUR:hr}:%{MINUTE:mn}:%{SECOND:sec})" ]
      }
      # Here, we apparently need to do that in 2 steps, instead the correct date is not set in the timestamp variable.
      mutate { add_field => [ "date_msg", "%{+YYYY}-%{+MM}-%{jour} %{hr}:%{mn}:%{sec}" ] }
      mutate { replace => { timestamp => "%{date_msg}" } }
      date {
        # KWA : do we need to change this if the servers are configured with another locale ? Apparently, it changes very little things (only the month/week parsing (Cf. https://www.elastic.co/guide/en/logstash/current/plugins-filters-date.html)) ; from my understanding, it should only be useful for parsing syslog RFC3164 logs.
        locale => fr
        #timezone => "UTC"
        match => [ "timestamp" , "yyyy-MM-dd HH:mm:ss" ]
        target => [ "@timestamp" ]
      }
  }
  # --------------------------------------------------
  if [type] =~ "elastic" {
      grok {
        match => [ "message", "(?:\[)%{TIMESTAMP_ISO8601:timestamp}(?:\])\[(?<err_level>[^ ]*)(?:%{SPACE:space}?)(?:\])\[(?<module_name>[^ ]*)(?:%{SPACE:space}?)(?:\])(?:%{SPACE:space}?)\[(?<node_name>[^\]]*)(?:\])(?:%{SPACE:space}?)(?:%{GREEDYDATA:msg_info})" ]
        overwrite => [ "timestamp" ]
      }
  }
  if [type] =~ "curator" {
      grok {
        match => [ "message", "%{TIMESTAMP_ISO8601:timestamp}(?:%{SPACE:space}?)(?<err_level>[^ ]*)(?:%{SPACE:space}?)(?<module_name>[^ ]*)(?:%{SPACE:space}?)(?:%{SPACE:space}?)(?:%{GREEDYDATA:msg_info})" ]
        overwrite => [ "timestamp" ]
      }
  }
  # --------------------------------------------------
  if "_grokparsefailure" not in [tags] {
     if [type] =~ "logback" or [type] =~ "elastic" or [type] =~ "curator" or [type] =~ "scripts" {
         date {
           locale => fr
           #timezone => "UTC"
           #format date fichier log vitam 2016-08-05 23:35:16,573
           match => [ "timestamp" , "yyyy-MM-dd HH:mm:ss,SSS" ]
           # KWA Note : Why @timestamp and not directly timestamp ? Is it a system field (notably for Kibana, apparently) ?
           target => [ "@timestamp" ]
         }
     }
     mutate {
        remove_field => [
                #"syslog_timestamp", "syslog_message",
                "syslog_program",
                "space", "date_msg", "mois", "jour", "hr", "mn", "sec",
                "severity", "facility", "facility_label", "severity_label", "priority",
                "request",
                "request_info",
                "path",
                "path_file",
                "path_log",
                "message",
                "msg_data",
                #"timestamp",
                "@version"
                           ]
     }
  }
}
# -----------------------------------------------------
output {
  if "_grokparsefailure" not in [tags] {
    if "drop" not in [tags] {
      if [type] =~ "logback" or [type] =~ "mongo" or [type] =~ "scripts" {
        elasticsearch {

          # KWA TODO: Prévoir un peu de tuning de perf ici
          #flush_size => 10000
          #idle_flush_time => 1
          #idle_flush_time => 2
          #workers => 20

          template => "{{logstash_confextra_dir}}/elasticsearch-template.json"
          index => "logstash-vitam-%{+YYYY.MM.dd}"
          # KWA TODO : use a join on the ansible group of elastisearch nodes (Cf. consul role)
          hosts => ["{{vitam_logserver_host}}:{{elasticsearch_port}}"]
        }
      } else {
        if [type_input] == "import_elastic" or [type_input] == "import_curator" {
          elasticsearch {
            template => "{{logstash_confextra_dir}}/elasticsearch-template.json"
            index => "logstash-logs-%{+YYYY.MM.dd}"
            # KWA TODO : use a join on the ansible group of elastisearch nodes (Cf. consul role)
            hosts => ["{{vitam_logserver_host}}:{{elasticsearch_port}}"]
          }
        } else {
          elasticsearch {
            template => "{{logstash_confextra_dir}}/elasticsearch-template.json"
            index => "logstash-%{type}-%{+YYYY.MM.dd}"
            # KWA TODO : use a join on the ansible group of elastisearch nodes (Cf. consul role)
            hosts => ["{{vitam_logserver_host}}:{{elasticsearch_port}}"]
          }
        }
      }
    }
  } else {
    if [type] =~ "logback" or [type] =~ "elastic" or [type] =~ "mongo" or [type] =~ "scripts" {
        elasticsearch {
          index => "logstash-failure-%{+YYYY.MM.dd}"
          hosts => ["{{vitam_logserver_host}}:{{elasticsearch_port}}"]
        }
     }
  }
  # -----------------------------------------------------
  # Following is for debug purpose only
  #stdout {
  #  #debug => debug_format => "json"
  #  codec => rubydebug
  #}
}
